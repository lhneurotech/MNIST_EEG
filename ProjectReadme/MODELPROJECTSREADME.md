# EEG to MNIST Prediction using Transformer Architecture
## Project Overview

This project aims to build a Transformer-based model to predict MNIST numbers from EEG data segments. Given the complexity of both EEG data and the Transformer architecture, tasks have been broken down into specific components.

## Components & Tasks

### 1. Encoder

#### 1.1 Embedding layer

* **Task 1.1.1**: Implement an embedding strategy suitable for EEG data
* **Task 1.1.2**: Implement a positional embedding strategy

#### 1.2 Encoder

* **Task 1.2.1**: Define the multi-head attention mechanism
* **Task 1.2.2**: Implement the feed-forward neural networks for the encoder
* **Task 1.2.3**: Implement a method to Stack any number of encoder layers

### 2. Decoder

#### 2.1 Decoder? 

* **Task 2.1.1**: Do you think a Decoder is necessary for a classification task?

#### 2.2 Output layer

* **Task 2.2.1**: Implement the final output layer to predict MNIST numbers
* **Task 2.2.2**: Ensure the output is suitable for a classification task

